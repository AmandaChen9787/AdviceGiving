---
title: "Flattering Advice: Avoiding Disappointment as a Driver of Gender Discrimination"
shorttitle        : "Flattering Advice"
author: 
  - name          : "Amanda Chen"
    affiliation   : "1"
    corresponding : yes    
    email         : "zchengj@connect.ust.hk"
    address       : "Department of Management, The Hong Kong University of Science and Technology, Hong Kong"
  - name          : "David Hagmann"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "The Hong Kong University of Science and Technology"
note : |
  `r format(Sys.time(), "%e %B, %Y")`
authornote : |
  The authors are grateful to a lot of things.
abstract: |
  
  This is an interesting project! Please read through it!
keywords          : "Gender difference, Advice, Interpresonal relationship"
wordcount         : "4,362"
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
header-includes:
   - \usepackage{setspace, appendix, placeins, booktabs, tikz, siunitx}
   - \usepackage{tabularx, epigraph, float, colortbl, tabu, pdflscape}
   - \usetikzlibrary{positioning}
   - \raggedbottom
   - \usepackage[maxfloats=256]{morefloats}
   - \maxdeadcycles=1000
bibliography      : bibliography.bib
output: 
  # papaja::apa6_docx
  papaja::apa6_pdf
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
always_allow_html: true
---


```{r setup, include=FALSE}
library(magrittr)

knitr::opts_chunk$set(fig.width = 8, fig.height = 4, cache = F, echo = F,
                      fig.align = 'center',warning = FALSE, message = FALSE)

study1_math50 <- read.csv(here::here("Data/Cleaned Data/S1_Math50_cleaned.csv"))
study1_time1All <- readRDS(here::here("Data/Study1_time1All.Rds"))
study1_time1 <- readRDS(here::here("Data/Study1_time1.Rds"))
study1_time2_giver <- readRDS(here::here("Data/Study1_time2_giver.Rds"))
study1_time2_giver50 <- readRDS(here::here("Data/Study1_time2_giver50.Rds"))
study1_advice <- readRDS(here::here("Data/Study1_time2_advice.Rds"))
study1_time3 <- readRDS(here::here("Data/Study1_time3.Rds"))

```

# Introduction
Seeking and giving advice can be challenging conversations to navigate. Advice seekers may be embarrassed to ask for help [@Benabou2022], and they may fail to do so if the information they receive could lead to disappointment [@Gill2012]. Indeed, people prefer advisers who withhold unpleasant information [@Shalvi2019] and punish those who tell them things they do not want to hear [@John2019]. People appear to engage in strategic selection of advice to avoid unpleasant information and protect the utility they get from positive beliefs about themselves and the future they anticipate [@Loewenstein2018; @Loewenstein1987]. But do advice givers anticipate and consider the hedonic cost of honest information to the recipient? While previous work has emphasized the instrumental value of advice (e.g., satisfaction with a decision or accuracy of a prediction), we focus on the motivations of the person providing advice.

Research on advice has largely focused on settings that do not tackle belief utility. For example, participants may have been asked to estimate the number of balls in an urn or choose from pairs of lotteries [@Benjamin2015]. Advisers have been shown to be motivated to give accurate advice [@Jonas2003], and engage in perspective-taking to meet the preferences of the advisee [@Kray2000; @Kray1999]. When they are uncertain, they prefer that advisees do not rely (extensively) on their recommendations [@Ache2020]. Thus, it seems that people are concerned about providing good advice even when they are not incentivized for the recipient’s decision quality.

In many real settings, however, advice incorporates beliefs about the recipient. Someone recommending an expensive restaurant conveys that they think the advisee is wealthy enough to afford an expensive meal, and someone recommending a student pursue a PhD in economics assumes that they are intellectually capable. Conversely, telling a poorly performing student to switch to a less-demanding major conveys a belief that the student is not capable of improving. Thus, the painful truth may prevent someone from making a bad decision, it may come at a cost to the recipient and damage the relationship between the adviser and advisee. We propose that advisers take these costs into account, even when the interactions are entirely anonymous and they are incentivized for the quality of the advisee’s decision.

Specifically, we focus on the signaling value of advice: what does the advice imply about the adviser's perception of the advisee? We propose that advisees have expectations about the advice they receive and treat this as a reference point [@Koszegi2006]. Advice that falls short of these expectations creates disappointment, while advice that exceeds them is flattering and creates positive interpersonal benefits because it creates a perception that the adviser holds the advisee in high esteem. Such interpersonal considerations may be as important, if not more important, to the adviser than the instrumental value of the advice: the adviser directly experiences a souring relationship, but may suffer little and only much delayed consequences if an advisee makes a poor life decision.

Our account therefore predicts that people receive advice that is more flattering and suggestive of inflated ability and optimistic outcomes than would advice in the absence of such interpersonal considerations. However, we propose that this is not true for all advisees equally. Presenting flattering advice requires the formation of beliefs about the advisee's expectations (reference point). @Exley2022 show that men are more overconfident than women, conditional on equal performance and that people anticipate but fail to incorporate this difference into their decisions. An adviser who seeks to avoid disappointment and incorporates expectations into their advice would then present more favorable advice to male advisees than to female advisees (see Figure 1 for an illustration). This provides a novel account for a well-known finding that men receive more aspirational advice than do women [@Kanze2018].

Notably, when it pays to have accurate beliefs, such flattering advice would come at a cost: while men would be more likely to aim higher, they would also suffer greater costs from aiming too high and failing to meet the objective. This may seem at odds with a gender gap favoring men. However, observational data often miss people who were unsuccessful. For example, job candidates who ask for a high starting salary may be less likely to receive a job offer, but will also have a higher starting salary conditional on receiving an offer.

In a large, preregistered experiment (n = 2,000), we show that advisers consider the expectations of advisees, even when such expectations do not provide informative insights for the underlying decision-making problem. We find that as a result, showing expectations (but not gender) to advisers leads men to receive more aspirational advice than women. Incorporating expectations leads to worse advice, even when advisers are incentivized based on the outcome of the advisee’s decision. In a second preregistered experiment (n = 700), we show that advisers respond to interpersonal considerations. They advice people that they are more attractive than they believe them to be, and inflate to a greater extent when incentivized for likability rather than accuracy. Advisees indeed reward such flattering advice, finding the advisers more likeable and no less trustworthy than those who provide more accurate advice.

# Open Science Statement
We report all sample sizes, data exclusions, all manipulations, and all measures in the studies. Screen captures of the experimental materials are available in the Supplemental Information. The complete data, code to reproduce all statistical analyses and figures in the manuscript, as well as the preregistration reports are available via OSF. All our studies were preregistered on AsPredicted.org. (will do this later)

# Study 1
We begin by examining whether advisers take the expectations of advisees into account, even when those expectations are not relevant to the quality of advice. In a three-stage experiment, we first recruit a sample of advisees and ask them to complete a mathematics quiz. After answering all ten multiple-choice questions, participants are anchored to low or high performance expectations and asked to guess how many questions they answered correctly. Next, we recruit advice givers and show them either only the true performance of the advisee, or the true performance and how many questions they guessed they answered correctly. Advisers are tasked with recommending whether the advisee should compete against a group of high-performers or a group of low-performers on the mathematics quiz. Finally, we invite the advisees back and show them the advice they have received. Advisees then pick whether to compete against high or low performers, based on their past performance on the quiz. Our key hypothesis is that advisees who were anchored to high expectations would be more likely to receive advice to compete against the high performer group.

## Methods
```{r} 

subgroup_averages <- data.frame(subgroup = numeric(), avg_score = numeric())
set.seed(807)
# loop through 1000 iterations of randomly selecting 10 participants and computing their average score
for (i in 1:1000) {
  # randomly select 10 participants
  subgroup <- sample(study1_math50$ProlificID, size = 5, replace = FALSE)
  # compute the average score for the selected participants
  subgroup_avg <- mean(study1_math50$Score[study1_math50$ProlificID %in% subgroup])
  # add the subgroup and its average score to the dataframe
  subgroup_averages <- rbind(subgroup_averages, data.frame(subgroup = i, avg_score = subgroup_avg))
}

p5 <- quantile(subgroup_averages$avg_score, probs = 0.05)
p95 <- quantile(subgroup_averages$avg_score, probs = 0.95)

sortedScore <- sort(study1_math50$Score, decreasing = TRUE)
LP_lower <- sortedScore[50]
LP_upper <- sortedScore[31]
HP_lower <- sortedScore[20]
HP_upper <- sortedScore[1]

```

We first recruited a sample of `r scales::comma(dim(study1_math50)[1])` participants from Prolific to complete a 10-question multiple choice mathematics quiz. The questions were taken from a paper-version of the ASVAB standardized exam, such that answers were not available online. Participants had 5 minutes to answer the quiz and were paid 10 cents for each correctly answered question. We define the top 20 scorers as the “High Performers” and the bottom 20 scorers as the “Low Performers.” On average, participants answered `r round(mean(study1_math50$Score),2)` questions correctly, High Performers scored between `r HP_lower` and `r HP_upper`, and Low Performers scored between `r LP_lower` and `r LP_upper`. We further simulated 1,000 pairings of groups of five participants, with the 5 th percentile of groups scoring an average of `r p5` and the 95 th percentile scoring an average of `r p95`. We use these participants as the competitors for the advisees in our main experiment, and the average group scores to anchor expectations.

Next, we recruited `r scales::comma(dim(study1_time1All)[1])` participants in Stage 1 of our main experiment, for the role of advisees. To arrive at a gender-balanced sample, we dropped the last two male participants to complete the survey, ending up with a sample of 500 men and 500 women ($M_{\text{Age}}$ = `r mean(study1_time1$Age)`,50\% Female,). Participants completed the same 10-item mathematics quiz as the earlier participants and were informed that their performance would affect their bonus earnings in a follow-up study to be conducted a few days later. After completing the quiz, we informed them of the average score of a group of five participants from the preliminary survey. We randomly assigned them to learn about the 5 th percentile of groups, which scored `r p5`. (“Low Expectations” treatment) or the 95 th percentile of groups, which scored `r p95` (“High Expectations” treatment). Participants then made a guess (unincentivized) about how many questions they think they answered correctly. The survey concluded with basic demographic questions.

We then recruited `r scales::comma(dim(study1_time2_giver)[1])`  participants for Stage 2, placing them in the role of advisers. We began by informing them of the mathematics quiz that participants in the preliminary study and Stage 1 had completed, and informed them of the average score in the preliminary study. Advisers had to recommend whether an advisee should compete against the Low Performers or the High Performers (we used these terms in the survey). We anticipated that being told to compete against High Performers is more flattering. Advisees would earn 30 cents if their score was equal to or higher than a randomly selected member of the Low Performer group and they chose to compete against this group, or 50 cents if they performed as well or better than a randomly selected member of the High Performer group. If they scored lower, they would not receive any bonus.

Advisers were randomly assigned to one of two treatments. In the “Baseline” treatment, they only observed the score of the advisee on the mathematics quiz. In the “Expectation” treatment, they observed the score as well as the advisee’s guess for how many questions they answered correctly. Note that since the outcome is determined only by the past score on the quiz, the advisee’s guess is immaterial to which group they should compete against, and in neither treatment did they receive any demographic information about their advisee. Participants gave recommendations to 10 advisees, which unbeknownst to them were five men and five women matched to have identical performance on the test. They were informed that if their advice was shown to a participant who returned for the follow-up survey, they would receive the identical bonus as that participant. The survey also concluded with basic demographic questions.
Finally, we invited participants from Stage 1 back for the follow-up survey. Following our preregistration, we kept the survey open for 7 days. In total, `r scales::comma(dim(study1_time3)[1])`  participants (`r scales::comma(dim(study1_time3[study1_time3$Gender == 'Male',])[1])` men, `r scales::comma(dim(study1_time3[study1_time3$Gender == 'Female',])[1])` women) returned. The brief survey reminded them of the task they completed in Stage 1, informed them that other participants from Prolific had observed their real score and given them advice against which group to compete against, and finally were reminded about how many questions they guessed they had answered correctly. Importantly, they were not informed of their true score or the score of the groups they could compete against. Participants then observed the advice from a randomly selected adviser and made their decision.

```{r}
library("papaja")
# Fit linear regression model
study1_advice <- study1_advice %>% dplyr::mutate(ExpShow = ifelse(Condition == 'E',1,0))

model1 <- lm(Guess ~ Gender + count, data = study1_time1)
model1_apa <- apa_print(model1)
```

## Results
We begin by examining the performance of the Stage 1 participants. Because our treatment takes place after participants complete the mathematics quiz, we would not expect a difference in performance across the Low and High Expectations treatments. Indeed, the two groups scored no different from one another (`r round(mean(study1_time1[study1_time1$Condition == "Low",]$count),2)` and `r round(mean(study1_time1[study1_time1$Condition == "High",]$count),2)` for Low and High Expectations treatments, respectively, `r papaja::apa_print(t.test(count ~ Condition, data = study1_time1, var.equal = T))$statistic`). The expectations treatment did, however, affect how well they thought they performed. Participants in the “Low Expectations” treatment guessed a score of `r round(mean(study1_time1[study1_time1$Condition == "Low",]$count),2)` vs. `r round(mean(study1_time1[study1_time1$Condition == "High",]$Guess),2)` in the High Expectations treatment (`r papaja::apa_print(t.test(Guess ~ Condition, data = study1_time1, var.equal = T))$statistic`), showing that the manipulation was successful. Contrary to our expectations, we did find a gender difference in performance: men scored `r round(mean(study1_time1[study1_time1$Gender == "Male",]$count),2)` on average, while women scored `r round(mean(study1_time1[study1_time1$Gender == "Female",]$count),2)` (`r papaja::apa_print(t.test(count ~ Gender, data = study1_time1, var.equal = T))$statistic`). This difference, however, does not affect the interpretation of our findings, which will rely on an interaction of gender with an experimental treatment for Stage 2 participants. Consistent with this difference, men thought they answered more questions correctly than did women (`r round(mean(study1_time1[study1_time1$Gender == "Male",]$Guess),2)` vs `r round(mean(study1_time1[study1_time1$Gender == "Female",]$Guess),2)`, `r papaja::apa_print(t.test(Guess ~ Gender, data = study1_time1, var.equal = T))$statistic`).

We conducted an OLS regression with the performance and gender of participants and found that with the same performance, male participants are more confident than their female counterparts (`r model1_apa$estimate['GenderMale']`, `r model1_apa$statistic['GenderMale']`).

```{r study1regs}
study1_advice <- study1_advice %>% dplyr::mutate(TreatmentHigh = ifelse(Treatment == 'High',1,0))

modelsummary::modelsummary(list(`(1)` = lm(Advice ~ TreatmentHigh + Performance , data = study1_advice %>% dplyr::filter(ExpShow == 1)),
                                `(2)` = lm(Advice ~ ExpShow*Gender , data = study1_advice),
                                `(3)` = lm(ExpectedBonus ~ ExpShow*Gender, data = study1_advice)),
                           vcov = c(~GiverID, ~GiverID,~GiverID),
                            coef_map = c("TreatmentHigh" = "High Expectation",
                                         "Performance" = "Performance",
                                         "ExpShow" = "Expectation Shown",
                                        "GenderMale" = "Advisee Male",
                                        "ExpShow:GenderMale" = "Expectation x Male",
                                        "(Intercept)" = "Constant"),
                           stars = T,
                           gof_map = list(list('raw' = 'nobs', 'clean' = 'N', 'fmt' = 0)),
                           caption = "Column 1 displays the advice given to compete against a High Performance group based on whether the advisees were primed with low or high expectations and the expectation level shown to the advisor. Column 2 displays the advice given to compete against a High Performance group based on the gender of the targets and the expectation level shown to the advisor. Column 3 displays the expected bonus of the advice received based on the gender of the targets and the expectation level shown to the advisor. All results are clustered at the level of the advisor and include standard errors.") %>%
  kableExtra::kable_styling(latex_option = "scale_down")

```

```{r study1genderdiff, fig.cap ="Male participants are more likely to be recommended to compete against the High Performer group than female counterparts, holding performance the same."} 
library(dplyr)
library(ggplot2)
library(magrittr)

ProbHG <- study1_advice %>%
  group_by(Condition, Performance, Gender) %>%
  dplyr::summarize(avg_hardGroup = mean(Advice))


df_exp <- subset(ProbHG, Condition == "E")
df_none <- subset(ProbHG, Condition == "None")

ggplot(df_exp, aes(x = Performance, y = avg_hardGroup, color = Gender)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"),se = FALSE) +
  geom_smooth(data = df_none, aes(x = Performance, y = avg_hardGroup),
              method = "glm", method.args = list(family = "binomial"), color = "grey", se = FALSE) +
  scale_color_manual(values = c("#FFA500", "#4169E1", "grey"),
                     labels = c("Female_withExpectation", "Male_withExpectation", "NoExpectation")) +
  scale_y_continuous(name = "P(Compete Against High Performance)") +
  scale_x_continuous(name = "Performance") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(colour = "grey"),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.size = unit(1.2, "lines"),
        legend.text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5, size = 20))


```
Next, we examine whether advisers took the advisee’s expectations into account. Column 1 of Table \@ref(tab:study1regs) reports a linear probability model on advising to compete against the High Performer group for advisers in the “Expectations” treatment. Because each adviser was paired with ten advisees, we cluster standard errors at the adviser level. Consistent with our prediction, we find that advisees in the High Expectation condition are more likely to be recommended to compete against the High Performer Group, but the result is not significant.

To test whether expectations can lead to more flattering advice for men than for women, we report a linear probability model with advice to compete against the high group as the outcome measure, and the advisee’s gender, whether expectations were shown to the adviser, and the interaction of the two (Column 2 of Table \@ref(tab:study1regs)). As predicted, we find a significant interaction effect: men are more likely to be advised to compete against the High Performers when expectations are shown than when they are hidden (p < 0.001; see Figure \@ref(fig:study1genderdiff),).

To examine the quality of advice, we computed the expected bonus earnings for all recommendations. We made this calculation by assuming that participants adopted the advice and competed against the recommended competitor group. We then compared their scores with all 20 possible competitors one by one and calculated the bonus accordingly to determine the expected bonus earnings. We conducted an OLS regression with whether givers see the expectation (Baseline or Information condition) and the gender of the Advisee (Female or Male), as well as their interaction. Supporting our theory, male participants will receive less bonus given advice they receive (b = -0.005, p < .01; see also Column 3 of Table \@ref(tab:study1regs)

```{r}
study1_time3 <- study1_time3 %>% dplyr::mutate(AdviseeConditionHigh = ifelse(AdviseeCondition =='High',1,0))

model2 <- lm(bonus ~ AdviseeConditionHigh + AdviseeScore, data = study1_time3)
model2_apa <- apa_print(model2)

model3 <- lm(Bet ~ Gender + AdviseeScore, data = study1_time3)
model3_apa <- apa_print(model3)

```
Finally, we look at the actual earnings of advisees. Although participants in the High Expectation condition earned less, this difference is not significant (`r model2_apa$estimate['AdviseeConditionHigh']`, `r model2_apa$statistic['AdviseeConditionHigh']`).

We found that males are more competitive to the extent that they are more likely to choose to compete against High Performer group (`r model3_apa$estimate['GenderMale']`, `r model3_apa$statistic['GenderMale']`), holding performance the same.

## Discussion
In Study 1, we discovered that although advisers are encouraged to provide accurate advice, they still take into account the expectations of advisees, which are irrelevant to the utility of the advice. Due to the fact that males tend to be more confident than their equally competent female counterparts, they are more likely to be recommended to compete against high-performance competitors. However, when advice is given to match the advisees' expectations, these flattering recommendations result in lower expected bonus earnings for male participants.

# Study 2
In the previous study, we found that individuals take into account the self-expectations of others when providing advice. In this study, we aimed to investigate whether people inflate advice due to the concern for interpersonal benefits. We hypothesized that individuals may exaggerate their advice in order to be viewed more favorably by the recipients. This study was preregistered on AsPredicted (https://aspredicted.org/45P_93G).

```{r}

study2_time1 <- readRDS(here::here("Data/Study2_time1.Rds"))
study2_time2 <- readRDS(here::here("Data/Study2_time2.Rds"))
study2_time3 <- readRDS(here::here("Data/Study2_time3.Rds"))

```

## Method
We recruited `r scales::comma(dim(study2_time1)[1])` workers from Prolific (`r round(mean(study2_time1$gender == "Female")*100)`\% Female, $M_{\text{Age}}$ = `r mean(study2_time1$age)`) to upload selfies and another `r scales::comma(dim(study2_time2)[1])` (`r round(mean(study2_time2$gender == "Female")*100)`\% Female, $M_{\text{Age}}$ = `r mean(study2_time2$age)`) participants to rate selfies regarding physical attractiveness and give advice. 
Study 2 consists of three surveys. At time 1, we invited participants to upload photos of themselves (selfies) to be rated by other participants on attractiveness. We assigned each participant to a group of 10 participants of their own gender. We also asked participants to predict how they rank in this 10-person group regarding physical attractiveness. To create a gender-balanced sample, among all `r dim(study2_time1 %>% dplyr::filter(gender == 'Female'))[1]` female participants who uploaded selfies, we select the first 100 and only include them in the following time 2 and time 3 studies. That is, in total we have 200 recipients. Next, we recruited `r scales::comma(dim(study2_time2)[1])` new participants and asked them to rank the attractiveness of one such group of 10 participants of the opposite gender. After they did so, we showed them the photo of the participant they ranked as the 7th most attractive (that is, the 4th least attractive) and reminded them that they had ranked this person at 7th place. We (truthfully) informed them that this participant would have a chance to earn a bonus if they accurately guessed their rank based on the aggregated ratings of all participants who had ranked this group. Participants were then randomly assigned to an “accuracy” or a “likability” treatment. In the accuracy treatment, they received a bonus identical to that of the advisee. In the “likability” treatment, their bonus depended on how the advisee rated them in terms of likability, after observing only the rank that they had advised the advisee to guess. At time 3, we invited the participants who uploaded selfies to return for the third stage of the experiment. Here, they observed the rank proposed by an adviser and made an incentivized bet on how they ranked across all of those who rated their photo. We then asked participants to evaluate the adviser based on their likability and warmth on a 5-point scale. The latter dimension was measured with 4-item scale adopted from Fiske, Cuddy, and Glick (2007).

## Open Science Statement
We report all manipulations, measures, and data exclusion in this and the following study. 
The preregistration reports, screenshots of all experimental materials, and 
the analysis code to replicate all statistical analyses and figures are
available on the Open Science Framework (will do this later).

## Results
```{r study2advice, fig.cap="Advice for 7th most attractive participants"}
library(dplyr)
library(ggplot2)

count <- study2_time2 %>%
  group_by(Condition) %>%
  summarize(number = dplyr::n()) 

advice_avg <- study2_time2 %>%
  group_by(Condition, Advice) %>%
  summarize(n = dplyr::n()) %>%
  dplyr::mutate(conditionCount = ifelse(Condition == 'Likable',count[count$Condition == 'Likable',]$number,count[count$Condition == 'Accurate',]$number)) %>%
  dplyr::mutate(percent = n/conditionCount*100) %>% 
  dplyr::select(Condition,Advice,percent)
  

# Create a facet plot with condition as the facet variable and gender as the line color
ggplot(advice_avg, aes(x = Advice, y = percent, color = Condition)) +
  geom_line(linewidth = 0.75)+
  scale_color_manual(values = c("#FFA500", "#4169E1"),
                      labels = c("Accurate", "Likable")) +
  scale_y_continuous(name = "% of participants") +
  scale_x_continuous(name = "Advised Rank") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(colour = "grey"),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.size = unit(1.2, "lines"),          
        legend.text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5, size = 20))

```

First, in line with our prediction, participants advised a lower rank (that is, more attractive) in the likeability treatment than the accuracy treatment, $M_{\text{Likable}}$  =`r round(mean(study2_time2[study2_time2$Condition == "Likable",]$Advice),2)` and $M_{\text{Accurate}}$ = `r round(mean(study2_time2[study2_time2$Condition == "Accurate",]$Advice),2)`, `r papaja::apa_print(t.test(Advice ~ Condition, data = study2_time2, var.equal = T))$statistic`. As seen in Figure \@ref(fig:study2advice), notably, even in the accuracy treatment, participants advised a lower rank than they themselves had provided (`r papaja::apa_print(t.test(study2_time2[study2_time2$Condition == "Accurate",]$Advice, mu = 7, var.equal = T))$statistic`). This suggests that advisers inferred that advising someone that they were more attractive would make the adviser appear more likable and therefore provided advice that communicated a more favorable impression of the participants’ attractiveness.

```{r study2regs}

modelsummary::modelsummary(list(`(1)` = lm(likable ~ Advice, data = study2_time3),
                                `(2)` = lm(warmAvg ~ Advice, data = study2_time3)),
                           vcov = c("classical", "classical"),
                           coef_map = c("Advice" = "Advised Rank",
                                        "(Intercept)" = "Constant"),
                           stars = T,
                           gof_map = list(list('raw' = 'nobs', 'clean' = 'N', 'fmt' = 0)),
                           caption = "When individuals receive advice that implies a high level of attractiveness (lower rank), they tend to perceive the advice giver as more likable (Column 1) and warm (Column 2).") %>%
  kableExtra::kable_styling(latex_option = "scale_down")

```

```{r study2rating, fig.cap="The evaluation of advice givers"}

rating <- study2_time3 %>%
  group_by(Advice) %>%
  summarise(Likable = mean(likable), Warm = mean(warmAvg), Trust = mean(trustAvg))

ggplot(rating, aes(x = Advice)) +
  geom_line(aes(y = Likable, color = "Likable"), linewidth = 0.75) +
  geom_line(aes(y = Warm, color = "Warm"), linewidth = 0.75) +
  geom_line(aes(y = Trust, color = "Trust"), linewidth = 0.75) +
  scale_color_manual(values = c("Likable" = "#FFA500", "Warm" = "#4169E1", "Trust" = "grey"),
                      labels = c("Likable", "Warm", "Trust")) +
  scale_y_continuous(name = "Rating") +
  scale_x_continuous(name = "Advised Rank") +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.border = element_blank(),
        axis.line = element_line(colour = "grey"),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.key.size = unit(1.2, "lines"),          
        legend.text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5, size = 20))


```

Of the 200 advisees, `r round(dim(study2_time3)[1]/200*100,2)`% returned for a follow-up survey, provided their incentivized guess, and evaluated advisers on likeability and warmth. As seen in Figure \@ref(fig:study2rating), in line of our predictions, we found that advisees who suggested that the advisee was more attractive were indeed rated as more likeable (Column 1 of Table \@ref(tab:study2regs) ), and warmer (Column 2 of Table \@ref(tab:study2regs) ). Notably, however, we were not powered to do a comparison across the two experimental groups.

# References

```{=tex}
\begingroup
\singlespacing
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\noindent
```
::: {#refs}
:::

```{=tex}
\endgroup
```
